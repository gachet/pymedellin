{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Digital Image Treatment\n",
    "\n",
    "OpenCV is one of the most popular libraries for DIT, it was originally wrote in C but since some time ago we can find Python bindings that let us to use with the simplied pythonic synthaxis.\n",
    "\n",
    "Let's begin to play some with the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CV2**: The OpenCV python binding library, it will let us to access to all the functions available in the library\n",
    "* **Numpy**: Numpy is an efficient matrix manipulation library, we will see that all our images will be treated as matrixes or vectors so we need a library to manipulate them.\n",
    "* **Imutils**: An useful image transformation library.\n",
    "* **Matplotlib**: As this library can be useful for different purposes, we will use it mainly for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load an image\n",
    "\n",
    "image = cv2.imread(\"teach_images/yiyo_pereza.png\") # Loads color image\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did I print? What those numbers mean?\n",
    "\n",
    "Most of the computers interpret an image as an array of color representation of 8 bits, this means, a number between 0 and 255. What you see in the last print statemen is this representation splitted into three different color channels: Red, Green and Blue, represented by a matrix that stores the pixels color representation of the image, this is what your computer 'sees' and that is why artificial vision applications requires some of math to be developed. For our fortune, OpenCV treats with it for us !!\n",
    "\n",
    "<img src=\"teach_images/image_representation.png\">\n",
    "\n",
    "The RGB color space means that you have a 8-bit unsigned representation for every color, below you can find some examples:\n",
    "\n",
    "\n",
    "* <img src=\"teach_images/rgb_255_0_0.png\", width=\"20px\" align=\"left\" top=\"10px\" bottom=\"10px\"> **Pure Red** [255, 0, 0]\n",
    "\n",
    "* <img src=\"teach_images/rgb_0_255_0.png\" width=\"20px\" align=\"left\" top=\"10px\" bottom=\"10px\"> **Pure Green** [0, 255, 0]\n",
    "* <img src=\"teach_images/rgb_255_255_0.png\" width=\"20px\" align=\"left\" top=\"10px\" bottom=\"10px\"> **Mix of Red and Green** [255, 255, 0]\n",
    "* <img src=\"teach_images/rgb_0_127_127.png\" width=\"20px\" align=\"left\" top=\"10px\" bottom=\"10px\"> **Attenuate mix of Green and Blue** [0, 127, 127]\n",
    "\n",
    "Keeping this in mind, it means that the last pixel from Yiyo's picture ```[207 221 220]``` has this color:\n",
    "\n",
    "* <img src=\"teach_images/rgb_207_221_220.png\" width=\"20px\" align=\"left\" top=\"10px\" bottom=\"10px\"> ** Yiyo's last pixel** [207 221 220]\n",
    "\n",
    "Finally, most of applications manages the color space as **RGB** but OpenCV manages it as **BGR**, so please **keep this in mind** as it will be very important in the incoming lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show the image to see how this array 'looks'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the first signal is plotted in a 'strange' way?**\n",
    "\n",
    "Images are treated as an array of three channels: Red, Green and Blue. Opencv treats with them as BGR while matplot treats with them as RGB.\n",
    "\n",
    "Let's solve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.axis('off')\n",
    "plt.imshow(image_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice, now let's split the channels\n",
    "\n",
    "image = cv2.imread(\"teach_images/yiyo_pereza.png\")\n",
    "blue, green, red = cv2.split(image)\n",
    "\n",
    "# Original one\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "# Splitted channels\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(131)\n",
    "plt.axis('off')\n",
    "plt.title('blue')\n",
    "plt.imshow(blue)\n",
    "plt.subplot(132)\n",
    "plt.axis('off')\n",
    "plt.title('green')\n",
    "plt.imshow(green)\n",
    "plt.subplot(133)\n",
    "plt.title('red')\n",
    "plt.axis('off')\n",
    "plt.imshow(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split another image\n",
    "image = cv2.imread(\"teach_images/jerico_2.png\")\n",
    "blue, green, red = cv2.split(image)\n",
    "\n",
    "# Original one\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "# Splitted channels\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(131)\n",
    "plt.axis('off')\n",
    "plt.title('blue')\n",
    "plt.imshow(blue)\n",
    "plt.subplot(132)\n",
    "plt.axis('off')\n",
    "plt.title('green')\n",
    "plt.imshow(green)\n",
    "plt.subplot(133)\n",
    "plt.title('red')\n",
    "plt.axis('off')\n",
    "plt.imshow(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split another image\n",
    "image = cv2.imread(\"teach_images/rose.png\")\n",
    "blue, green, red = cv2.split(image)\n",
    "\n",
    "# Original one\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "# Splitted channels\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(131)\n",
    "plt.axis('off')\n",
    "plt.title('blue')\n",
    "plt.imshow(blue)\n",
    "plt.subplot(132)\n",
    "plt.axis('off')\n",
    "plt.title('green')\n",
    "plt.imshow(green)\n",
    "plt.subplot(133)\n",
    "plt.title('red')\n",
    "plt.axis('off')\n",
    "plt.imshow(red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main RGB split channels are mask, look that the red channel would serve us to get most of the information of the rose's petals. We will review the mask concept later in the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grayspace Images\n",
    "\n",
    "From our previous job, we know that an image is represented as a RGB matrix, but to process a three columns matrix can be computationally costly and because of this many applications transforms the image in a vector array of 'gray' pixels intensities.\n",
    "\n",
    "Be careful, a grayscale space vector does not mean black or white, it is a new representation of the RGB space:\n",
    "\n",
    "Y = 0.299 x R + 0.587 x G + 0.114 x B\n",
    "\n",
    "due to the cones and receptors in our eyes, we are able to perceive nearly 2x the amount of green than red. And similarly, we notice over twice the amount of red than blue. Thus, we make sure to account for this when converting from RGB to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a grayscale image\n",
    "\n",
    "image = cv2.imread(\"teach_images/yiyo_pereza.png\")\n",
    "\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(121)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(122)\n",
    "plt.title('gray')\n",
    "plt.axis('off')\n",
    "plt.imshow(gray, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the gray array\n",
    "\n",
    "print(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the shapes of both color and gray\n",
    "\n",
    "print(image.shape)\n",
    "print(gray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Images\n",
    "\n",
    "One of the main application of the grayscale images is to create binary images, that contains **only** two possible pixel values: 0 or 255. The main application of binary images is mask creation to extract a region of interest of the image based on the number of pixels that are equals to zero or higher than zero.\n",
    "\n",
    "Let's create some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our objective will be to extract a mask with only the letters and number of a license plate\n",
    "\n",
    "plate = cv2.imread(\"teach_images/license_plate.png\")\n",
    "gray = cv2.cvtColor(plate, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(121)\n",
    "plt.title('Original Plate')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(plate, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(122)\n",
    "plt.title('Gray Plate')\n",
    "plt.axis('off')\n",
    "plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "\n",
    "Thresholding is one of the most common (and basic) segmentation techniques in computer vision and it allows us to separate the foreground (i.e. the objects that we are interested in) from the background of the image.\n",
    "\n",
    "For the example, we will use a simple thresholding, We must specify a threshold value T. All pixel intensities below T are set to 255. And all pixel intensities greater than T are set to 0.\n",
    "\n",
    "Keep in mind that our main objetive is to extract the letter and numbers of the plate, so we want to extract intensities of 255 which are 'pure' black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look some representations of some aparts of the image\n",
    "\n",
    "# The top of the image, which is only yellow\n",
    "print(gray[0:10])\n",
    "\n",
    "plt.imshow(gray[0:10], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look some representations of some aparts of the image\n",
    "\n",
    "# One of the letters (mainly black)\n",
    "print(gray[40: 110, 110:150])\n",
    "\n",
    "plt.imshow(gray[40: 110, 110:150], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look some representations of some aparts of the image\n",
    "\n",
    "# One of the noisy region\n",
    "print(gray[100: 170, 80:150])\n",
    "plt.subplot(121)\n",
    "plt.title('Original Plate')\n",
    "plt.imshow(cv2.cvtColor(plate[100: 170, 80:150], cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(122)\n",
    "plt.title('Gray Plate')\n",
    "plt.imshow(gray[100: 170, 80:150], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply a threshold\n",
    "\n",
    "# If a pixel value is greater than our threshold (in this case,\n",
    "# 213), we set it to be BLACK, otherwise it is WHITE.\n",
    "# REMEMBER: Black regions are in the order of 213\n",
    "(T, thresh_1) = cv2.threshold(gray, 213, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# If a pixel value is greater than our threshold (in this case,\n",
    "# 212), we set it to be BLACK, otherwise it is WHITE.\n",
    "(T, thresh_2) = cv2.threshold(gray, 212, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# If a pixel value is greater than our threshold (in this case,\n",
    "# 80), we set it to be BLACK, otherwise it is WHITE.\n",
    "(T, thresh_3) = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# If a pixel value is greater than our threshold (in this case,\n",
    "# 40), we set it to be BLACK, otherwise it is WHITE.\n",
    "(T, thresh_4) = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Greater than 213')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_1, cmap='gray')\n",
    "plt.subplot(222)\n",
    "plt.title('Greater than 212')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_2, cmap='gray')\n",
    "plt.subplot(223)\n",
    "plt.title('Greater than 80')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_3, cmap='gray')\n",
    "plt.subplot(224)\n",
    "plt.title('Greater than 80')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_4, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular application, a threshold of 40 takes off most of our noise, nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilation and Erosion\n",
    "\n",
    "Morphological transformations have a wide array of uses, i.e. :\n",
    "\n",
    "* Removing noise\n",
    "* Isolation of individual elements and joining disparate elements in an image.\n",
    "* Finding of intensity bumps or holes in an image\n",
    "\n",
    "Let's look some of them.\n",
    "\n",
    "* **Dilation**: As its name suggests, it 'dilates' a binary region transforming black pixels into white pixels\n",
    "\n",
    " <img src=\"teach_images/dilation.png\">\n",
    "\n",
    "* **Erosion**: The opposite to dilation, it transforms white pixels into black pixels.\n",
    "\n",
    "<img src=\"teach_images/dilation.png\">\n",
    "\n",
    "* **Opening**: Opening is just another name of erosion followed by dilation\n",
    "\n",
    "<img src=\"teach_images/opening.png\">\n",
    "\n",
    "* **Closing**: Closing is reverse of Opening, Dilation followed by Erosion\n",
    "\n",
    "<img src=\"teach_images/closing.png\">\n",
    "\n",
    "Now that you know the basics of image transformation, lets eliminate the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use an opening transformation to eliminate most of the noise\n",
    "\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "opening = cv2.morphologyEx(thresh_4, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Original thresholded')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_4, cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.title('Closing')\n",
    "plt.axis('off')\n",
    "plt.imshow(opening, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, most of the noise was taked off!! Now let's begin with a masking job to extract ROIS from our original image\n",
    "\n",
    "# Masking\n",
    "\n",
    "'Masking' is the process of extract a Region of Interest from our images, this is basically approached using a bitwise-and operation:\n",
    "```\n",
    "    0101\n",
    "AND 0011\n",
    "  = 0001\n",
    "```\n",
    "In OpenCV, what we will have is a mask with only two possible values: 0 or 255 (remember, a binary image). Let's look with a concrete example from our initial splitted RGB splitted rose. The main goal is to extract its petals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split another image\n",
    "rose = cv2.imread(\"teach_images/rose.png\")\n",
    "blue, green, red = cv2.split(rose)\n",
    "\n",
    "# Original one\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(rose, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "# Splitted channels\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(131)\n",
    "plt.axis('off')\n",
    "plt.title('blue')\n",
    "plt.imshow(blue)\n",
    "plt.subplot(132)\n",
    "plt.axis('off')\n",
    "plt.title('green')\n",
    "plt.imshow(green)\n",
    "plt.subplot(133)\n",
    "plt.title('red')\n",
    "plt.axis('off')\n",
    "plt.imshow(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red channel is the most useful for our purposes\n",
    "\n",
    "(T, thresh_rose) = cv2.threshold(red, 40, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(rose)\n",
    "plt.subplot(132)\n",
    "plt.title('Red')\n",
    "plt.axis('off')\n",
    "plt.imshow(red)\n",
    "plt.subplot(133)\n",
    "plt.title('thresholded')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_rose, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use an some morphological transformations to eliminate most of the noise\n",
    "\n",
    "kernel_1 = np.ones((50,50),np.uint8)\n",
    "opening_rose = cv2.morphologyEx(thresh_rose, cv2.MORPH_OPEN, kernel_1)\n",
    "kernel_2 = np.ones((5,5),np.uint8)\n",
    "closing = cv2.morphologyEx(opening_rose, cv2.MORPH_CLOSE, kernel_1)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(231)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(rose)\n",
    "plt.subplot(232)\n",
    "plt.title('Red')\n",
    "plt.axis('off')\n",
    "plt.imshow(red)\n",
    "plt.subplot(233)\n",
    "plt.title('thresholded')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_rose, cmap='gray')\n",
    "plt.subplot(234)\n",
    "plt.title('Opening')\n",
    "plt.axis('off')\n",
    "plt.imshow(opening_rose, cmap='gray')\n",
    "plt.subplot(235)\n",
    "plt.title('Closing')\n",
    "plt.axis('off')\n",
    "plt.imshow(closing, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's implement a masking!!\n",
    "clone = rose.copy()\n",
    "masked = cv2.bitwise_and(clone, clone, mask=closing)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(rose)\n",
    "plt.subplot(222)\n",
    "plt.title('Red')\n",
    "plt.axis('off')\n",
    "plt.imshow(red)\n",
    "plt.subplot(223)\n",
    "plt.title('Mask')\n",
    "plt.axis('off')\n",
    "plt.imshow(closing, cmap='gray')\n",
    "plt.subplot(224)\n",
    "plt.title('Masked')\n",
    "plt.axis('off')\n",
    "plt.imshow(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, isn't it? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contours\n",
    "\n",
    "There is another basic Image processing concept to review: Contours. Contours are simply the outlines of an object in an image. If the image is simple enough, we might be able to get away with using the grayscale image as an input. if not, we will need to apply some transformation and artificial vision techniques to obtain a properly objetive image as we made with our rose masking.\n",
    "\n",
    "Let's define an initial goal, we want to know in a tetris game how many pieces we need to play at time:\n",
    "\n",
    "<img src=\"teach_images/tetris_goal.png\">\n",
    "\n",
    "For the example above, there are three pieces to be played properly to keep our 'live' in the game. Let's begin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the image\n",
    "\n",
    "tetris = cv2.imread(\"teach_images/tetris_1.png\")\n",
    "gray = cv2.imread(\"teach_images/tetris_1.png\", 0)\n",
    "\n",
    "# Applies a threshold\n",
    "(T, thresh_tetris) = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Applies a closing\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "closing = cv2.morphologyEx(thresh_tetris, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Find contours \n",
    "cnts = cv2.findContours(closing.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[1]\n",
    "clone = gray.copy()\n",
    "clone = cv2.cvtColor(clone, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# draw the contours\n",
    "\n",
    "cv2.drawContours(clone, cnts, -1, (0, 0, 255), 2)\n",
    "print(\"Found {} contours\".format(len(cnts)))\n",
    "\n",
    "# Shows initial results\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(231)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(tetris, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(232)\n",
    "plt.title('Gray')\n",
    "plt.axis('off')\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Thresholded')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_tetris, cmap='gray')\n",
    "plt.subplot(234)\n",
    "plt.title('Closing')\n",
    "plt.axis('off')\n",
    "plt.imshow(closing, cmap='gray')\n",
    "plt.subplot(235)\n",
    "plt.title('Contours')\n",
    "plt.axis('off')\n",
    "plt.imshow(clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look that we have drawn all the contours of our image, but we have found five of them, how can we know which of them are tetris pieces?\n",
    "\n",
    "Let's look some more concepts:\n",
    "\n",
    "### Area: \n",
    "\n",
    "The number of pixels that reside inside the contour outline. We will expect a fixed max and min area for our tetris pieces.\n",
    "\n",
    "### Aspect Ratio\n",
    "\n",
    "The actual definition of the a contour’s aspect ratio is as follows:\n",
    "\n",
    "```\n",
    "aspect ratio = image width / image height\n",
    "```\n",
    "\n",
    "We will expect this aspect ratio: \n",
    "\n",
    "```[0.2, 0.4] and [0.6, 1.7]```\n",
    "\n",
    "Let's add just some line to inclue the area and the aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the image\n",
    "\n",
    "tetris = cv2.imread(\"teach_images/tetris_1.png\")\n",
    "gray = cv2.imread(\"teach_images/tetris_1.png\", 0)\n",
    "\n",
    "# Applies a threshold\n",
    "(T, thresh_tetris) = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Applies a closing\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "closing = cv2.morphologyEx(thresh_tetris, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Find contours \n",
    "contours = cv2.findContours(closing.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[1]\n",
    "clone = gray.copy()\n",
    "clone = cv2.cvtColor(clone, cv2.COLOR_GRAY2BGR)\n",
    "cnts = []\n",
    "\n",
    "for (i, c) in enumerate(contours):\n",
    "    area = cv2.contourArea(c)\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    checked = 0\n",
    " \n",
    "    aspect_ratio = w / float(h)\n",
    "    \n",
    "    checked = (area>400 and area<700) \n",
    "    checked = checked and ((aspect_ratio>=0.2  and aspect_ratio<=0.4) or (aspect_ratio>=0.6 and aspect_ratio<=1.8))\n",
    "    \n",
    "    print(\"contour number: {} area: {} aspect_ratio: {}\".format(i, area, aspect_ratio))\n",
    "    print(checked)\n",
    "    \n",
    "    if checked:\n",
    "        cnts.append(c)\n",
    "\n",
    "# draw the contours\n",
    "cv2.drawContours(clone, cnts, -1, (0, 0, 255), 2)\n",
    "# Shows initial results\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(231)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(tetris, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(232)\n",
    "plt.title('Gray')\n",
    "plt.axis('off')\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Thresholded')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh_tetris, cmap='gray')\n",
    "plt.subplot(234)\n",
    "plt.title('Closing')\n",
    "plt.axis('off')\n",
    "plt.imshow(closing, cmap='gray')\n",
    "plt.subplot(235)\n",
    "plt.title('Contours')\n",
    "plt.axis('off')\n",
    "plt.imshow(clone)\n",
    "\n",
    "# Prints the result\n",
    "\n",
    "print(\"The number of pieces to play is {}\".format(len(cnts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Comparing\n",
    "\n",
    "Finally, let's apply all that we have seen for image comparing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image compare script\n",
    "\n",
    "template = cv2.imread(\"teach_images/pycon_template.png\")\n",
    "testing = cv2.imread(\"teach_images/pycon_testing.png\")\n",
    "\n",
    "template_gray = cv2.cvtColor(template, cv2.COLOR_RGB2GRAY)\n",
    "testing_gray = cv2.cvtColor(testing, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Applies a bitwise XOR operation that will return one only if some pixel is different\n",
    "xor = np.bitwise_xor(template_gray, testing_gray)\n",
    "ones = cv2.countNonZero(xor)\n",
    "\n",
    "print(ones)\n",
    "\n",
    "# Let's paint the differences, if there is any\n",
    "\n",
    "if ones > 0:\n",
    "    result = cv2.absdiff(template, testing)\n",
    "    gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray, 1, 255, 0)\n",
    "    \n",
    "    # Find contours\n",
    "    cnts = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[1]\n",
    "    cv2.drawContours(result, cnts, -1, (0, 0, 255), 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "plt.subplot(221)\n",
    "plt.title('Template')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(222)\n",
    "plt.title('Testing')\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(223)\n",
    "plt.title('Thresholded')\n",
    "plt.axis('off')\n",
    "plt.imshow(thresh, cmap='gray')\n",
    "plt.subplot(224)\n",
    "plt.title('Contours')\n",
    "plt.axis('off')\n",
    "plt.imshow(result, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!!! You have drawn the images differences.\n",
    "\n",
    "Hope you have enjoyed this lesson :)\n",
    "\n",
    "All the best,\n",
    "\n",
    "José García"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
